# Rush RocksDB Database Backend

[![Rust](https://img.shields.io/badge/rust-1.89%2B-blue.svg)](https://www.rust-lang.org)
[![Version](https://img.shields.io/badge/version-0.6.5-green.svg)](#)

**RocksDB Database Backend** es la implementaci√≥n de alto rendimiento para Rush-rs, proporcionando capacidades de almacenamiento extremadamente r√°pidas usando RocksDB como motor de base de datos, optimizado para workloads de escritura intensiva y aplicaciones de tiempo real.

## üöÄ Caracter√≠sticas Principales

### Motor de Alto Rendimiento
- **RocksDB** como motor de almacenamiento de Facebook/Meta
- **LSM-Tree architecture** optimizada para escrituras masivas
- **Compresi√≥n LZ4/Snappy** integrada para eficiencia de almacenamiento
- **Bloom filters** para b√∫squedas ultrarr√°pidas
- **Multi-threaded compaction** para rendimiento sostenido

### Optimizado para Escala
- **Write throughput** superior a 500K ops/segundo
- **Read latency** submilisegundo en datasets grandes
- **Concurrent access** con m√∫ltiples threads sin bloqueos
- **Memory management** avanzado con block cache configurable
- **Crash recovery** autom√°tica con WAL (Write-Ahead Log)

### Seguridad y Robustez
- **Iterator safety** con eliminaci√≥n completa de c√≥digo unsafe
- **Memory safety** garantizada con Rust safe code
- **Error handling** exhaustivo sin panics en producci√≥n
- **Atomic operations** para consistencia de datos
- **Checksum validation** para integridad de datos

## üèóÔ∏è Arquitectura

```
RocksDbManager
‚îú‚îÄ‚îÄ Database Instance      # Instancia principal de RocksDB
‚îú‚îÄ‚îÄ Column Families       # Separaci√≥n l√≥gica de datos
‚îú‚îÄ‚îÄ Iterator Engine       # Sistema de iteraci√≥n seguro
‚îú‚îÄ‚îÄ Batch Operations      # Escrituras at√≥micas en lote
‚îú‚îÄ‚îÄ Compaction Manager    # Gesti√≥n autom√°tica de compactaci√≥n
‚îî‚îÄ‚îÄ Cache System         # Cache de bloques configurable
    ‚îú‚îÄ‚îÄ Block Cache      # Cache LRU para bloques de datos
    ‚îú‚îÄ‚îÄ Table Cache      # Cache de metadatos de tablas
    ‚îî‚îÄ‚îÄ Compression     # LZ4/Snappy seg√∫n configuraci√≥n
```

## üöÄ Inicio R√°pido

### Configuraci√≥n B√°sica

```rust
use rocksdb_db::*;
use rush_store::database::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    // 1. Crear manager de RocksDB
    let manager = RocksDbManager::new("./high_performance_db")?;

    // La base de datos se configura autom√°ticamente con:
    // - Compresi√≥n LZ4 para m√°ximo rendimiento
    // - Block cache optimizado
    // - Multi-threaded compaction
    // - Bloom filters habilitados
    // - Write-ahead logging para durabilidad

    // 2. Crear collection para eventos de alta frecuencia
    let mut events_collection = manager.create_collection(
        "user_events",     // Nombre de la column family
        "user_12345"       // Prefijo para particionado
    )?;

    // 3. Escrituras de alta velocidad
    let events = vec![
        ("login", b"{\"timestamp\": \"2025-01-01T10:00:00Z\", \"ip\": \"192.168.1.1\"}"),
        ("page_view", b"{\"page\": \"/dashboard\", \"duration\": 1500}"),
        ("action", b"{\"type\": \"button_click\", \"element\": \"save_btn\"}"),
        ("logout", b"{\"session_duration\": 3600}"),
    ];

    // Batch insert para m√°ximo rendimiento
    for (event_type, data) in &events {
        let key = format!("{}_{}", chrono::Utc::now().timestamp_nanos(), event_type);
        events_collection.put(&key, data)?;
    }

    println!("Inserted {} events at high speed", events.len());

    // 4. Lecturas ultrarr√°pidas
    let start = std::time::Instant::now();

    // Iterar sobre eventos (ordenados autom√°ticamente)
    let mut count = 0;
    for (key, value) in events_collection.iter(false) {
        count += 1;
        if count <= 5 {
            println!("Event {}: {} -> {} bytes",
                     count, key, value.len());
        }
    }

    let duration = start.elapsed();
    println!("Read {} events in {:?} ({:.0} events/sec)",
             count, duration, count as f64 / duration.as_secs_f64());

    Ok(())
}
```

### High-Throughput Writes

```rust
use rocksdb_db::*;
use std::time::Instant;
use rayon::prelude::*;

#[tokio::main]
async fn main() -> Result<(), Box<dyn std::error::Error>> {
    let manager = RocksDbManager::new("./throughput_test")?;

    // Configurar para m√°ximo throughput de escritura
    let mut batch_collection = manager.create_collection("batch_data", "high_throughput")?;

    let num_records = 100_000;
    println!("Writing {} records...", num_records);

    let start = Instant::now();

    // Escrituras masivas paralelas
    let records: Vec<(String, Vec<u8>)> = (0..num_records)
        .into_par_iter()
        .map(|i| {
            let key = format!("record_{:08}", i);
            let value = format!("data_payload_{}_{}_with_some_content_to_test_compression",
                               i, chrono::Utc::now().timestamp_nanos()).into_bytes();
            (key, value)
        })
        .collect();

    // Batch write para m√°xima eficiencia
    for (key, value) in &records {
        batch_collection.put(key, value)?;
    }

    let duration = start.elapsed();
    let throughput = num_records as f64 / duration.as_secs_f64();

    println!("Write performance:");
    println!("  Records: {}", num_records);
    println!("  Duration: {:?}", duration);
    println!("  Throughput: {:.0} records/second", throughput);
    println!("  Average latency: {:.2} Œºs/record",
             duration.as_micros() as f64 / num_records as f64);

    // Verificar compresi√≥n
    let db_size = std::fs::metadata("./throughput_test")?.len();
    let raw_size = records.iter().map(|(k, v)| k.len() + v.len()).sum::<usize>();
    let compression_ratio = db_size as f64 / raw_size as f64;

    println!("Compression:");
    println!("  Raw data size: {} MB", raw_size / 1_000_000);
    println!("  Compressed size: {} MB", db_size / 1_000_000);
    println!("  Compression ratio: {:.2}x", 1.0 / compression_ratio);

    Ok(())
}
```

## ‚ö° Optimizaciones de Rendimiento

### Configuraci√≥n Autom√°tica

```rust
// RocksDB se configura autom√°ticamente con:

impl RocksDbManager {
    pub fn new(path: &str) -> Result<Self, Error> {
        let mut opts = Options::default();

        // Optimizaciones para escrituras masivas
        opts.set_write_buffer_size(64 * 1024 * 1024);        // 64MB write buffer
        opts.set_max_write_buffer_number(3);                 // 3 write buffers
        opts.set_level_zero_file_num_compaction_trigger(8);   // Trigger compaction
        opts.set_level_zero_slowdown_writes_trigger(17);      // Slowdown threshold
        opts.set_level_zero_stop_writes_trigger(24);          // Stop threshold

        // Optimizaciones para lecturas
        opts.set_max_open_files(10000);                      // Cache de archivos
        opts.set_use_fsync(false);                           // fdatasync m√°s r√°pido
        opts.set_bytes_per_sync(1048576);                    // Sync incremental

        // Compresi√≥n optimizada
        opts.set_compression_type(CompressionType::Lz4);     // Compresi√≥n r√°pida
        opts.set_bottommost_compression_type(CompressionType::Zstd); // Compresi√≥n fuerte para datos antiguos

        // Block cache para lecturas r√°pidas
        let cache = Cache::new_lru_cache(256 * 1024 * 1024); // 256MB cache
        let mut block_opts = BlockBasedOptions::default();
        block_opts.set_block_cache(&cache);
        block_opts.set_bloom_filter(10, false);              // Bloom filter
        opts.set_block_based_table_factory(&block_opts);

        // Configuraci√≥n de compactaci√≥n
        opts.set_compaction_style(CompactionStyle::Level);
        opts.set_num_levels(7);
        opts.set_max_background_jobs(4);                     // Compactaci√≥n paralela

        let db = DB::open(&opts, path)?;

        Ok(RocksDbManager {
            opts,
            db: Arc::new(db),
        })
    }
}
```

### Batch Operations At√≥micas

```rust
use rocksdb::{WriteBatch, WriteOptions};

impl RocksDbManager {
    pub fn batch_write(&self, operations: Vec<BatchOperation>) -> Result<(), Error> {
        let mut batch = WriteBatch::default();

        for op in operations {
            match op {
                BatchOperation::Put { key, value } => {
                    batch.put(key, value);
                }
                BatchOperation::Delete { key } => {
                    batch.delete(key);
                }
                BatchOperation::Merge { key, value } => {
                    batch.merge(key, value);
                }
            }
        }

        // Opciones de escritura optimizadas
        let mut write_opts = WriteOptions::default();
        write_opts.set_sync(false);        // Async writes para velocidad
        write_opts.disable_wal(false);     // WAL habilitado para durabilidad

        self.db.write_opt(batch, &write_opts)
            .map_err(|e| Error::Store(format!("Batch write failed: {}", e)))?;

        Ok(())
    }
}

#[derive(Debug)]
pub enum BatchOperation {
    Put { key: Vec<u8>, value: Vec<u8> },
    Delete { key: Vec<u8> },
    Merge { key: Vec<u8>, value: Vec<u8> },
}

// Uso para m√°ximo rendimiento
let operations = vec![
    BatchOperation::Put {
        key: b"user:123:profile".to_vec(),
        value: b"{'name': 'John', 'email': 'john@example.com'}".to_vec()
    },
    BatchOperation::Put {
        key: b"user:123:settings".to_vec(),
        value: b"{'theme': 'dark', 'lang': 'en'}".to_vec()
    },
    BatchOperation::Delete {
        key: b"user:123:temp_data".to_vec()
    },
];

manager.batch_write(operations)?; // Escritura at√≥mica de todas las operaciones
```

### Iteraci√≥n Optimizada

```rust
// Sistema de iteraci√≥n seguro eliminando completamente c√≥digo unsafe
pub struct RocksDbIterator {
    current_batch: std::vec::IntoIter<(String, Vec<u8>)>,
    db: Arc<DB>,
    prefix: String,
    reverse: bool,
    batch_size: usize,
    last_key: Option<String>,
}

impl RocksDbIterator {
    pub fn new(db: Arc<DB>, prefix: String, reverse: bool) -> Self {
        Self {
            current_batch: Vec::new().into_iter(),
            db,
            prefix,
            reverse,
            batch_size: 1000, // Batch de 1000 para eficiencia
            last_key: None,
        }
    }

    fn load_next_batch(&mut self) -> bool {
        let mut opts = ReadOptions::default();
        opts.set_iterate_upper_bound(self.get_upper_bound());

        let iter = self.db.iterator_opt(
            IteratorMode::From(self.get_start_key().as_bytes(), Direction::Forward),
            opts
        );

        let mut batch = Vec::with_capacity(self.batch_size);
        let mut count = 0;
        let mut should_skip = self.last_key.is_some();

        for item in iter {
            if count >= self.batch_size {
                break;
            }

            match item {
                Ok((key, value)) => {
                    if let Ok(key_str) = String::from_utf8(key.to_vec()) {
                        // Skip hasta encontrar la posici√≥n despu√©s de last_key
                        if should_skip {
                            if let Some(ref last) = self.last_key {
                                if key_str == *last {
                                    should_skip = false;
                                }
                                continue;
                            }
                        }

                        if key_str.starts_with(&self.prefix) {
                            let trimmed_key = if self.prefix.is_empty() {
                                key_str.clone()
                            } else {
                                key_str[self.prefix.len()..].to_string()
                            };

                            batch.push((trimmed_key, value.to_vec()));
                            count += 1;
                            self.last_key = Some(key_str);
                        }
                    }
                }
                Err(_) => break,
            }
        }

        if self.reverse {
            batch.reverse();
        }

        let has_data = !batch.is_empty();
        self.current_batch = batch.into_iter();
        has_data
    }
}

impl Iterator for RocksDbIterator {
    type Item = (String, Vec<u8>);

    fn next(&mut self) -> Option<Self::Item> {
        // Intentar obtener del batch actual
        if let Some(item) = self.current_batch.next() {
            return Some(item);
        }

        // Batch actual agotado, cargar siguiente
        if self.load_next_batch() {
            self.current_batch.next()
        } else {
            None
        }
    }
}
```

## üìä Monitoring y M√©tricas

### Estad√≠sticas de RocksDB

```rust
impl RocksDbManager {
    pub fn get_statistics(&self) -> DatabaseStatistics {
        let stats = self.db.property_value("rocksdb.stats").unwrap_or_default();

        DatabaseStatistics {
            // M√©tricas de escritura
            write_throughput: self.get_write_throughput(),
            write_latency_p50: self.get_write_latency_percentile(50),
            write_latency_p99: self.get_write_latency_percentile(99),

            // M√©tricas de lectura
            read_throughput: self.get_read_throughput(),
            read_latency_p50: self.get_read_latency_percentile(50),
            read_latency_p99: self.get_read_latency_percentile(99),

            // M√©tricas de almacenamiento
            total_size_bytes: self.get_total_size(),
            compression_ratio: self.get_compression_ratio(),

            // M√©tricas de compactaci√≥n
            compaction_pending: self.is_compaction_pending(),
            background_errors: self.get_background_errors(),

            // Cache hits
            block_cache_hit_ratio: self.get_block_cache_hit_ratio(),

            raw_stats: stats,
        }
    }

    pub fn print_detailed_stats(&self) {
        let stats = self.get_statistics();

        println!("RocksDB Performance Statistics:");
        println!("  Write Throughput: {:.0} ops/sec", stats.write_throughput);
        println!("  Read Throughput: {:.0} ops/sec", stats.read_throughput);
        println!("  Write Latency P50: {:.2} ms", stats.write_latency_p50);
        println!("  Write Latency P99: {:.2} ms", stats.write_latency_p99);
        println!("  Read Latency P50: {:.2} ms", stats.read_latency_p50);
        println!("  Read Latency P99: {:.2} ms", stats.read_latency_p99);
        println!("  Total Size: {} MB", stats.total_size_bytes / 1_000_000);
        println!("  Compression Ratio: {:.2}x", stats.compression_ratio);
        println!("  Block Cache Hit Rate: {:.1}%", stats.block_cache_hit_ratio * 100.0);
        println!("  Background Errors: {}", stats.background_errors);
    }
}
```

### Profiling de Rendimiento

```rust
use std::sync::atomic::{AtomicU64, Ordering};
use std::time::{Duration, Instant};

pub struct PerformanceProfiler {
    operation_counts: [AtomicU64; 4], // Put, Get, Delete, Iter
    operation_times: [AtomicU64; 4],  // Tiempo total en nanosegundos
}

impl PerformanceProfiler {
    pub fn new() -> Self {
        Self {
            operation_counts: [AtomicU64::new(0), AtomicU64::new(0), AtomicU64::new(0), AtomicU64::new(0)],
            operation_times: [AtomicU64::new(0), AtomicU64::new(0), AtomicU64::new(0), AtomicU64::new(0)],
        }
    }

    pub fn time_operation<F, R>(&self, op_type: OperationType, f: F) -> R
    where
        F: FnOnce() -> R,
    {
        let start = Instant::now();
        let result = f();
        let duration = start.elapsed();

        let index = op_type as usize;
        self.operation_counts[index].fetch_add(1, Ordering::Relaxed);
        self.operation_times[index].fetch_add(duration.as_nanos() as u64, Ordering::Relaxed);

        result
    }

    pub fn get_stats(&self) -> PerformanceStats {
        let mut stats = PerformanceStats::default();

        for (i, &count) in self.operation_counts.iter().enumerate() {
            let count = count.load(Ordering::Relaxed);
            let total_time = self.operation_times[i].load(Ordering::Relaxed);

            if count > 0 {
                let avg_latency_nanos = total_time / count;
                let avg_latency = Duration::from_nanos(avg_latency_nanos);

                match i {
                    0 => { // Put
                        stats.put_count = count;
                        stats.put_avg_latency = avg_latency;
                    }
                    1 => { // Get
                        stats.get_count = count;
                        stats.get_avg_latency = avg_latency;
                    }
                    2 => { // Delete
                        stats.delete_count = count;
                        stats.delete_avg_latency = avg_latency;
                    }
                    3 => { // Iter
                        stats.iter_count = count;
                        stats.iter_avg_latency = avg_latency;
                    }
                    _ => {}
                }
            }
        }

        stats
    }
}

#[derive(Debug, Clone, Copy)]
pub enum OperationType {
    Put = 0,
    Get = 1,
    Delete = 2,
    Iter = 3,
}

// Integraci√≥n con RocksDbCollection
impl RocksDbCollection {
    pub fn put_with_profiling(&mut self, key: &str, data: &[u8]) -> Result<(), Error> {
        self.profiler.time_operation(OperationType::Put, || {
            self.put(key, data)
        })
    }

    pub fn get_with_profiling(&self, key: &str) -> Result<Vec<u8>, Error> {
        self.profiler.time_operation(OperationType::Get, || {
            self.get(key)
        })
    }
}
```

## üîÑ Compactaci√≥n y Mantenimiento

### Compactaci√≥n Manual

```rust
impl RocksDbManager {
    pub fn compact_range(&self, start: Option<&[u8]>, end: Option<&[u8]>) -> Result<(), Error> {
        self.db.compact_range(start, end);
        Ok(())
    }

    pub fn compact_all(&self) -> Result<(), Error> {
        // Compactar toda la base de datos
        self.compact_range(None, None)?;

        // Esperar a que termine la compactaci√≥n
        while self.is_compaction_pending() {
            std::thread::sleep(Duration::from_millis(100));
        }

        Ok(())
    }

    pub fn trigger_manual_compaction(&self) -> Result<(), Error> {
        // Forzar flush de memtables a SST files
        let mut flush_opts = FlushOptions::default();
        flush_opts.set_wait(true);
        self.db.flush_opt(&flush_opts)?;

        // Compactaci√≥n nivel por nivel para m√°xima eficiencia
        for level in 0..7 {
            self.db.compact_range_cf_opt(
                self.db.cf_handle("default").unwrap(),
                None,
                None,
                &CompactionOptions::new()
                    .set_target_level(level)
                    .set_exclusive_manual_compaction(false)
            );
        }

        Ok(())
    }
}
```

### Estrategias de Maintenance Autom√°tico

```rust
use tokio::time::{interval, Duration};

pub struct MaintenanceScheduler {
    db_manager: Arc<RocksDbManager>,
    compaction_interval: Duration,
    stats_interval: Duration,
}

impl MaintenanceScheduler {
    pub fn new(db_manager: Arc<RocksDbManager>) -> Self {
        Self {
            db_manager,
            compaction_interval: Duration::from_secs(3600), // 1 hora
            stats_interval: Duration::from_secs(300),       // 5 minutos
        }
    }

    pub async fn start_background_maintenance(&self) {
        let db_manager = Arc::clone(&self.db_manager);
        let compaction_interval = self.compaction_interval;

        // Tarea de compactaci√≥n peri√≥dica
        tokio::spawn(async move {
            let mut interval = interval(compaction_interval);

            loop {
                interval.tick().await;

                if db_manager.should_compact() {
                    match db_manager.compact_all() {
                        Ok(_) => {
                            tracing::info!("Scheduled compaction completed successfully");
                        }
                        Err(e) => {
                            tracing::error!("Scheduled compaction failed: {}", e);
                        }
                    }
                }
            }
        });

        // Tarea de reporte de estad√≠sticas
        let db_manager_stats = Arc::clone(&self.db_manager);
        let stats_interval = self.stats_interval;

        tokio::spawn(async move {
            let mut interval = interval(stats_interval);

            loop {
                interval.tick().await;

                let stats = db_manager_stats.get_statistics();
                tracing::info!(
                    "RocksDB stats - Write: {:.0} ops/s, Read: {:.0} ops/s, Size: {} MB, Cache hit: {:.1}%",
                    stats.write_throughput,
                    stats.read_throughput,
                    stats.total_size_bytes / 1_000_000,
                    stats.block_cache_hit_ratio * 100.0
                );

                // Alertas autom√°ticas
                if stats.write_latency_p99 > 100.0 {
                    tracing::warn!("High write latency detected: {:.2} ms", stats.write_latency_p99);
                }

                if stats.block_cache_hit_ratio < 0.8 {
                    tracing::warn!("Low cache hit ratio: {:.1}%", stats.block_cache_hit_ratio * 100.0);
                }
            }
        });
    }
}

// Uso
let maintenance = MaintenanceScheduler::new(Arc::clone(&db_manager));
maintenance.start_background_maintenance().await;
```

## üß™ Testing de Alto Rendimiento

### Load Testing

```rust
#[cfg(test)]
mod load_tests {
    use super::*;
    use std::sync::Arc;
    use tokio::task::JoinSet;
    use std::time::Instant;

    #[tokio::test]
    async fn test_concurrent_high_throughput() {
        let manager = Arc::new(RocksDbManager::new("./load_test_db").unwrap());
        let num_threads = 8;
        let operations_per_thread = 10_000;

        let mut join_set = JoinSet::new();

        let start = Instant::now();

        // Lanzar escritores concurrentes
        for thread_id in 0..num_threads {
            let manager_clone = Arc::clone(&manager);

            join_set.spawn(async move {
                let mut collection = manager_clone
                    .create_collection("load_test", &format!("thread_{}", thread_id))
                    .unwrap();

                let mut thread_start = Instant::now();

                for i in 0..operations_per_thread {
                    let key = format!("key_{}_{:06}", thread_id, i);
                    let value = format!(
                        "value_data_{}_{}_timestamp_{}_payload_with_sufficient_length_for_compression_testing",
                        thread_id, i, chrono::Utc::now().timestamp_nanos()
                    ).into_bytes();

                    collection.put(&key, &value).unwrap();

                    // Lectura cada 10 escrituras para simular workload mixto
                    if i % 10 == 0 && i > 0 {
                        let read_key = format!("key_{}_{:06}", thread_id, i - 1);
                        let _ = collection.get(&read_key);
                    }
                }

                let thread_duration = thread_start.elapsed();
                let thread_throughput = operations_per_thread as f64 / thread_duration.as_secs_f64();

                println!("Thread {} completed: {:.0} ops/sec", thread_id, thread_throughput);
                thread_throughput
            });
        }

        // Esperar a que completen todos los threads
        let mut total_throughput = 0.0;
        while let Some(result) = join_set.join_next().await {
            total_throughput += result.unwrap();
        }

        let total_duration = start.elapsed();
        let total_operations = num_threads * operations_per_thread;
        let overall_throughput = total_operations as f64 / total_duration.as_secs_f64();

        println!("\nLoad Test Results:");
        println!("  Threads: {}", num_threads);
        println!("  Operations per thread: {}", operations_per_thread);
        println!("  Total operations: {}", total_operations);
        println!("  Total duration: {:?}", total_duration);
        println!("  Overall throughput: {:.0} ops/sec", overall_throughput);
        println!("  Sum of thread throughputs: {:.0} ops/sec", total_throughput);

        // Verificar que el throughput es aceptable
        assert!(overall_throughput > 50_000.0,
                "Throughput too low: {:.0} ops/sec", overall_throughput);

        // Verificar estad√≠sticas de la base de datos
        let stats = manager.get_statistics();
        println!("\nDatabase Statistics:");
        println!("  Write latency P99: {:.2} ms", stats.write_latency_p99);
        println!("  Compression ratio: {:.2}x", stats.compression_ratio);
        println!("  Block cache hit rate: {:.1}%", stats.block_cache_hit_ratio * 100.0);
    }

    #[tokio::test]
    async fn test_memory_usage_under_load() {
        use sysinfo::{System, SystemExt, ProcessExt};

        let mut system = System::new_all();
        system.refresh_all();

        let initial_memory = system.process(sysinfo::get_current_pid().unwrap())
            .unwrap()
            .memory();

        let manager = RocksDbManager::new("./memory_test_db").unwrap();
        let mut collection = manager.create_collection("memory_test", "load").unwrap();

        // Escribir 100K records
        for i in 0..100_000 {
            let key = format!("memory_key_{:08}", i);
            let value = vec![0u8; 1024]; // 1KB por record
            collection.put(&key, &value).unwrap();

            if i % 10_000 == 0 {
                system.refresh_process(sysinfo::get_current_pid().unwrap());
                let current_memory = system.process(sysinfo::get_current_pid().unwrap())
                    .unwrap()
                    .memory();
                let memory_growth = current_memory - initial_memory;

                println!("After {} records: {} MB memory growth",
                         i, memory_growth / 1024);
            }
        }

        system.refresh_process(sysinfo::get_current_pid().unwrap());
        let final_memory = system.process(sysinfo::get_current_pid().unwrap())
            .unwrap()
            .memory();
        let total_growth = final_memory - initial_memory;

        println!("Total memory growth: {} MB", total_growth / 1024);
        println!("Memory per record: {} bytes", total_growth / 100_000);

        // Verificar que el uso de memoria es razonable
        assert!(total_growth < 500 * 1024, // Menos de 500MB
                "Memory usage too high: {} MB", total_growth / 1024);
    }
}
```

### Stress Testing

```rust
#[cfg(test)]
mod stress_tests {
    use super::*;

    #[tokio::test]
    async fn test_sustained_write_load() {
        let manager = RocksDbManager::new("./stress_test_db").unwrap();
        let mut collection = manager.create_collection("stress_test", "sustained").unwrap();

        let test_duration = Duration::from_secs(60); // 1 minuto de stress test
        let start = Instant::now();
        let mut operations = 0u64;

        while start.elapsed() < test_duration {
            let batch_start = Instant::now();

            // Lote de 1000 operaciones
            for i in 0..1000 {
                let key = format!("stress_key_{}_{}", operations, i);
                let value = format!("stress_data_{}_timestamp_{}",
                                   operations, chrono::Utc::now().timestamp_nanos()).into_bytes();

                collection.put(&key, &value).unwrap();
            }

            operations += 1000;
            let batch_duration = batch_start.elapsed();
            let batch_throughput = 1000.0 / batch_duration.as_secs_f64();

            if operations % 10_000 == 0 {
                println!("Sustained load: {} ops total, {:.0} ops/sec current batch",
                         operations, batch_throughput);
            }

            // Peque√±a pausa para evitar saturar completamente el sistema
            if batch_throughput > 100_000.0 {
                tokio::time::sleep(Duration::from_millis(1)).await;
            }
        }

        let total_duration = start.elapsed();
        let sustained_throughput = operations as f64 / total_duration.as_secs_f64();

        println!("\nSustained Load Test Results:");
        println!("  Duration: {:?}", total_duration);
        println!("  Total operations: {}", operations);
        println!("  Sustained throughput: {:.0} ops/sec", sustained_throughput);

        // Verificar que mantuvo un throughput m√≠nimo
        assert!(sustained_throughput > 10_000.0,
                "Sustained throughput too low: {:.0} ops/sec", sustained_throughput);

        // Verificar estad√≠sticas finales
        let stats = manager.get_statistics();
        assert!(stats.background_errors == 0, "Background errors detected: {}", stats.background_errors);
    }
}
```

## üîß Configuraci√≥n Avanzada

### Tuning para Diferentes Workloads

```rust
pub struct RocksDbConfig {
    pub write_buffer_size: usize,
    pub max_write_buffer_number: i32,
    pub block_cache_size: usize,
    pub compression_type: CompressionType,
    pub max_background_jobs: i32,
    pub workload_type: WorkloadType,
}

#[derive(Debug, Clone)]
pub enum WorkloadType {
    WriteHeavy,    // Optimizado para escrituras masivas
    ReadHeavy,     // Optimizado para lecturas frecuentes
    Balanced,      // Balance entre lectura y escritura
    AnalyticsSpan, // Optimizado para scans largos
}

impl RocksDbConfig {
    pub fn for_workload(workload: WorkloadType) -> Self {
        match workload {
            WorkloadType::WriteHeavy => Self {
                write_buffer_size: 128 * 1024 * 1024,  // 128MB buffer
                max_write_buffer_number: 6,             // M√∫ltiples buffers
                block_cache_size: 512 * 1024 * 1024,    // 512MB cache
                compression_type: CompressionType::Lz4,  // Compresi√≥n r√°pida
                max_background_jobs: 8,                  // M√°s threads compactaci√≥n
                workload_type: WorkloadType::WriteHeavy,
            },
            WorkloadType::ReadHeavy => Self {
                write_buffer_size: 32 * 1024 * 1024,    // 32MB buffer
                max_write_buffer_number: 2,              // Menos buffers
                block_cache_size: 1024 * 1024 * 1024,   // 1GB cache
                compression_type: CompressionType::Zstd, // Mejor compresi√≥n
                max_background_jobs: 2,                  // Menos compactaci√≥n
                workload_type: WorkloadType::ReadHeavy,
            },
            WorkloadType::Balanced => Self::default(),
            WorkloadType::AnalyticsSpan => Self {
                write_buffer_size: 64 * 1024 * 1024,
                max_write_buffer_number: 3,
                block_cache_size: 2048 * 1024 * 1024,   // 2GB cache para scans
                compression_type: CompressionType::Zstd,
                max_background_jobs: 4,
                workload_type: WorkloadType::AnalyticsSpan,
            },
        }
    }
}

impl RocksDbManager {
    pub fn with_config(path: &str, config: RocksDbConfig) -> Result<Self, Error> {
        let mut opts = Options::default();

        opts.set_write_buffer_size(config.write_buffer_size);
        opts.set_max_write_buffer_number(config.max_write_buffer_number);
        opts.set_max_background_jobs(config.max_background_jobs);
        opts.set_compression_type(config.compression_type);

        // Configuraciones espec√≠ficas por workload
        match config.workload_type {
            WorkloadType::WriteHeavy => {
                opts.set_level_zero_file_num_compaction_trigger(4);
                opts.set_level_zero_slowdown_writes_trigger(20);
                opts.set_level_zero_stop_writes_trigger(36);
                opts.set_disable_auto_compactions(false);
            }
            WorkloadType::ReadHeavy => {
                opts.set_level_zero_file_num_compaction_trigger(8);
                opts.optimize_for_point_lookup(config.block_cache_size as u64);
            }
            WorkloadType::AnalyticsSpan => {
                opts.optimize_level_style_compaction(config.write_buffer_size);
            }
            _ => {}
        }

        let cache = Cache::new_lru_cache(config.block_cache_size);
        let mut block_opts = BlockBasedOptions::default();
        block_opts.set_block_cache(&cache);
        opts.set_block_based_table_factory(&block_opts);

        let db = DB::open(&opts, path)?;

        Ok(RocksDbManager {
            opts,
            db: Arc::new(db),
        })
    }
}

// Uso especializado
let write_heavy_config = RocksDbConfig::for_workload(WorkloadType::WriteHeavy);
let write_optimized_db = RocksDbManager::with_config("./write_heavy_db", write_heavy_config)?;

let read_heavy_config = RocksDbConfig::for_workload(WorkloadType::ReadHeavy);
let read_optimized_db = RocksDbManager::with_config("./read_heavy_db", read_heavy_config)?;
```

## üìö API Reference

### Core Types

- **`RocksDbManager`** - Manager principal de RocksDB
- **`RocksDbCollection`** - Implementaci√≥n de Collection para RocksDB
- **`RocksDbIterator`** - Iterador seguro sin c√≥digo unsafe
- **`DatabaseStatistics`** - M√©tricas detalladas de rendimiento

### Configuration Types

- **`RocksDbConfig`** - Configuraci√≥n completa de RocksDB
- **`WorkloadType`** - Tipos de workload para optimizaci√≥n autom√°tica
- **`CompressionType`** - Tipos de compresi√≥n disponibles

### Performance Types

- **`PerformanceProfiler`** - Profiler de rendimiento integrado
- **`OperationType`** - Tipos de operaciones monitoreadas
- **`MaintenanceScheduler`** - Programador de mantenimiento autom√°tico

---

Este backend RocksDB proporciona rendimiento extremo para aplicaciones Rush-rs que requieren throughput masivo de escritura, baja latencia de lectura y escalabilidad horizontal. üöÄ‚ö°üìä